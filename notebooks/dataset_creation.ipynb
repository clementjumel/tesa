{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/clement/nltk_data'\n    - '/Users/clement/Code/clementjumel/tesa/.venv/nltk_data'\n    - '/Users/clement/Code/clementjumel/tesa/.venv/share/nltk_data'\n    - '/Users/clement/Code/clementjumel/tesa/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/Code/clementjumel/tesa/.venv/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/clementjumel/tesa/.venv/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/Users/clement/nltk_data'\n    - '/Users/clement/Code/clementjumel/tesa/.venv/nltk_data'\n    - '/Users/clement/Code/clementjumel/tesa/.venv/share/nltk_data'\n    - '/Users/clement/Code/clementjumel/tesa/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/q_/_rmfznmx4hxbdy3xvq197z9h0000gn/T/ipykernel_10108/2986171926.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtesa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstandard_parser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_annotations_arguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_task_arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtesa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatabase_creation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotation_task\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAnnotationTask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtesa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_annotations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfilter_annotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtesa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/clementjumel/tesa/tesa/database_creation/annotation_task.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwikipedia\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDisambiguationError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWikipediaException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtesa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatabase_creation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnyt_article\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArticle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtesa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatabase_creation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAnnotation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWikipedia\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/clementjumel/tesa/tesa/database_creation/nyt_article.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mToken\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m     \u001b[0mpunctuation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring_punctuation\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"''\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"``\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk_stopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/clementjumel/tesa/tesa/database_creation/nyt_article.py\u001b[0m in \u001b[0;36mToken\u001b[0;34m()\u001b[0m\n\u001b[1;32m    656\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mToken\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mpunctuation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring_punctuation\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"''\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"``\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m     \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk_stopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/clementjumel/tesa/.venv/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/clementjumel/tesa/.venv/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/clementjumel/tesa/.venv/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/clementjumel/tesa/.venv/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/clement/nltk_data'\n    - '/Users/clement/Code/clementjumel/tesa/.venv/nltk_data'\n    - '/Users/clement/Code/clementjumel/tesa/.venv/share/nltk_data'\n    - '/Users/clement/Code/clementjumel/tesa/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "del sys\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tesa.toolbox.parsers import standard_parser, add_annotations_arguments, add_task_arguments\n",
    "from tesa.database_creation.annotation_task import AnnotationTask\n",
    "from tesa.preprocess_annotations import filter_annotations\n",
    "from tesa.toolbox.utils import load_task\n",
    "from tesa.modeling.utils import format_context\n",
    "from os.path import join as path_join\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = standard_parser()\n",
    "add_annotations_arguments(ap)\n",
    "add_task_arguments(ap)\n",
    "args = ap.parse_args([\"--root\", \"..\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the annotations data (and first preprocessing step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the modeling task...\n",
      "Computing the annotated queries...\n",
      "Initial length of queries: 0.\n",
      "Object loaded from ../results/annotation_task/annotations/v2_0/task/queries_short.pkl.\n",
      "Object loaded from ../results/annotation_task/annotations/v2_1/task/queries.pkl.\n",
      "Object loaded from ../results/annotation_task/annotations/v2_2/task/queries.pkl.\n",
      "Final length of queries: 61056.\n",
      "Done. Elapsed time: 1s.\n",
      "\n",
      "Computing the annotations...\n",
      "Initial length of annotations: 0.\n",
      "batch_00 loaded from annotations/v2_0/results/batch_00_complete.csv\n",
      "Correcting \"n this article, Nevada and Ohio are discussed. The two American states...\" to \" The two American states...\"\n",
      "Correcting \"In this article, California and Oregon are discussed. The two neighboring states...\" to \" The two neighboring states...\"\n",
      "Correcting \"In this article, California and Oregon are discussed. The two West Coast states...\" to \" The two West Coast states...\"\n",
      "batch_01 loaded from annotations/v2_0/results/batch_01_complete.csv\n",
      "Discarding \"The\"\n",
      "Discarding \"The four people involved in the Rafferty and Parker murder case\"\n",
      "Discarding \"The two people involved in the accusations against Mr. Brookins and Mr. Hernandez\"\n",
      "Discarding \"The Canadian and American politicians\"\n",
      "Correcting \"THE CHESS CHAMPIONS\" to \"The chess champions\"\n",
      "Correcting \"THE WHITE AND BLACK\" to \"The white and black\"\n",
      "Discarding \"The white and black\"\n",
      "Discarding \"The North and South America countries\"\n",
      "Discarding \"Both are politlca entities\"\n",
      "batch_02 loaded from annotations/v2_1/results/batch_02_complete.csv\n",
      "Discarding \"Both groups have a military wing\"\n",
      "Discarding \"The financial/media concern\"\n",
      "Discarding \"Both countries are in Western Asia\"\n",
      "Correcting \"FOOTBALL TEAM\" to \"Football team\"\n",
      "Discarding \"Na\"\n",
      "batch_03 loaded from annotations/v2_1/results/batch_03_complete.csv\n",
      "Discarding \"The convicted Hynix managers/directors\"\n",
      "Discarding \"The areas that include or are included by Eurasia\"\n",
      "Discarding \"Evo Morales and Hugo Salvatierra\"\n",
      "Discarding \"The politician and the organ builder\"\n",
      "batch_04 loaded from annotations/v2_2/results/batch_04_complete.csv\n",
      "Discarding \"The country and its capital city\"\n",
      "Discarding \"The major united states city and the state nearby\"\n",
      "Discarding \"The judge and the woman he sentenced to jail\"\n",
      "Discarding \"Are in European territory\"\n",
      "Discarding \"The states that have seen declining enrollment in Medicaid\"\n",
      "batch_05 loaded from annotations/v2_2/results/batch_05_complete.csv\n",
      "Discarding \"Former attorneys and politicians\"\n",
      "Discarding \"The director/choreographers\"\n",
      "Discarding \"The writer/directors\"\n",
      "Final length of annotations: 2100.\n",
      "Done. Elapsed time: 1s.\n",
      "\n",
      "Done. Elapsed time: 2s.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "annotation_task = AnnotationTask(silent=args.silent,\n",
    "                                     results_path=path_join(args.root, args.annotations_path),\n",
    "                                     years=None,\n",
    "                                     max_tuple_size=None,\n",
    "                                     short=None,\n",
    "                                     short_size=None,\n",
    "                                     random=None,\n",
    "                                     debug=None,\n",
    "                                     random_seed=None,\n",
    "                                     save=None,\n",
    "                                     corpus_path=None)\n",
    "\n",
    "annotation_task.process_task(exclude_pilot=args.exclude_pilot)\n",
    "\n",
    "queries = annotation_task.queries\n",
    "annotations = annotation_task.annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_data = []\n",
    "for id_, annotation_list in annotations.items():\n",
    "    data = dict()\n",
    "    \n",
    "    query = queries[id_]\n",
    "    data[\"entities_type\"] = query.entities_type_\n",
    "    data[\"entities\"] = query.entities\n",
    "    data[\"summaries\"] = query.summaries\n",
    "    data[\"urls\"] = query.urls\n",
    "    data[\"title\"] = query.title\n",
    "    data[\"date\"] = query.date\n",
    "    data[\"context\"] = query.context\n",
    "    data[\"context_type\"] = query.context_type_\n",
    "    \n",
    "    for i, annotation in enumerate(annotation_list):\n",
    "        if annotation.answers:\n",
    "            data[f\"answer_{i}\"] = annotation.answers\n",
    "            \n",
    "    global_data.append(data)\n",
    "    \n",
    "df = pd.DataFrame(global_data)[[\"entities_type\",\"entities\",\"answer_0\",\"answer_1\",\"answer_2\",\"title\",\"date\",\"urls\",\"summaries\",\"context_type\",\"context\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../results/publication/dataset.csv\", index=False, mode=\"w\")\n",
    "pickle.dump(global_data, open(\"../results/publication/dataset.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the modeling task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task loaded from ../results/modeling_task/context-dependent-same-type_50-25-25_rs24_bs4_cf-v0_tf-v0.pkl.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task = load_task(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    loader = getattr(task, f\"{split}_loader\")\n",
    "    global_data = []\n",
    "    \n",
    "    for ranking_task in loader:\n",
    "        data = dict()\n",
    "        \n",
    "        first_input = ranking_task[0][0]\n",
    "        data[\"entities\"] = first_input[\"entities\"]\n",
    "        data[\"entities_type\"] = first_input[\"entities_type\"]\n",
    "        data[\"wiki_articles\"] = first_input[\"wiki_articles\"]\n",
    "        assert len(first_input[\"nyt_titles\"]) == 1\n",
    "        assert len(first_input[\"nyt_contexts\"]) == 1\n",
    "        data[\"nyt_title\"] = first_input[\"nyt_titles\"][0]\n",
    "        data[\"nyt_context\"] = first_input[\"nyt_contexts\"][0]\n",
    "        \n",
    "        candidates = []\n",
    "        labels = []\n",
    "        \n",
    "        for batch_inputs, batch_outputs in ranking_task:\n",
    "            candidates.extend(batch_inputs[\"choices\"])\n",
    "            labels.extend(list(batch_outputs.tolist()))\n",
    "            \n",
    "        data[\"candidates\"] = candidates\n",
    "        data[\"labels\"] = labels\n",
    "        \n",
    "        global_data.append(data)\n",
    "\n",
    "    df = pd.DataFrame(global_data)[[\"entities_type\",\"entities\",\"wiki_articles\",\"nyt_title\",\"nyt_context\",\"candidates\",\"labels\"]]\n",
    "    \n",
    "    df.to_csv(f\"../results/publication/ranking_task_{split}.csv\", index=False, mode=\"w\")\n",
    "    pickle.dump(global_data, open(f\"../results/publication/ranking_task_{split}.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "del sys\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from database_creation.database import Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = Database()\n",
    "database.process_task(assignment_threshold=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the number of valid answers (ie not bugs) per task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = [0, 0, 0, 0]\n",
    "valid_ids, bug_ids = [], []\n",
    "\n",
    "for id_, annotation_list in database.annotations.items():\n",
    "    cmpt = 0\n",
    "    \n",
    "    for annotation in annotation_list:\n",
    "        if not annotation.bug:\n",
    "            cmpt += 1\n",
    "    \n",
    "    valid[cmpt] += 1\n",
    "    \n",
    "    if cmpt >= 2:\n",
    "        valid_ids.append(id_)\n",
    "    else:\n",
    "        bug_ids.append(id_)\n",
    "\n",
    "print(\"Number of perfect/good examples: {}/{}\".format(valid[3], valid[2]))\n",
    "print(\"Number of bad/awful examples: {}/{}\".format(valid[1], valid[0]))\n",
    "print(\"Number of accepted tasks: {}, number of rejected: {} ({}% accepted)\".format(len(valid_ids), len(bug_ids), round(100*len(valid_ids)/(len(valid_ids) + len(bug_ids)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id_ in valid_ids:\n",
    "    print(database.queries[id_])\n",
    "    for annotation in database.annotations[id_]:\n",
    "        print(annotation)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bad results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id_ in bug_ids:\n",
    "    print(database.queries[id_])\n",
    "    for annotation in database.annotations[id_]:\n",
    "        print(annotation)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unique answers for each task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "tuple_answers = defaultdict(list)\n",
    "\n",
    "for id_ in valid_ids:\n",
    "    a = []\n",
    "    \n",
    "    for annotation in database.annotations[id_]:\n",
    "        if not annotation.bug:\n",
    "            a.extend(annotation.preprocessed_answers)\n",
    "\n",
    "    entities = ', '.join(sorted(database.queries[id_].entities))\n",
    "    tuple_answers[entities].append(a)\n",
    "            \n",
    "    print(entities, ' -> ', ', '.join(set([s+' ['+str(a.count(s))+']' if a.count(s)>1 else s for s in a])))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unique answers for each tuple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_answers = [(len([answer for answer_list in answers for answer in answer_list]),\n",
    "                   entities,\n",
    "                   answers)\n",
    "                  for entities, answers in tuple_answers.items()]\n",
    "sorted_answers = sorted(sorted_answers, reverse=True)\n",
    "\n",
    "for count, entities, answers in sorted_answers:\n",
    "    flattened_answers = [answer for answer_list in answers for answer in answer_list]\n",
    "    flattened_unique_answers = set(flattened_answers)\n",
    "    \n",
    "    print(entities, ' (unique/total answers {}/{})'.format(len(flattened_unique_answers), len(flattened_answers)))\n",
    "    \n",
    "    answers_counts = set([(flattened_answers.count(a), a) for a in flattened_answers])\n",
    "    sorted_answers_counts = sorted(answers_counts, reverse=True)\n",
    "    \n",
    "    print(', '.join([answer+' ['+str(count)+']' for count, answer in sorted_answers_counts]), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall most frequent answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_answers_dict = defaultdict(int)\n",
    "\n",
    "for _, _, answers in sorted_answers:\n",
    "    flattened_answers = [answer for answer_list in answers for answer in answer_list]\n",
    "    for answer in flattened_answers:\n",
    "        overall_answers_dict[answer] += 1\n",
    "\n",
    "overall_answers = [(count, answer) for answer, count in overall_answers_dict.items()]\n",
    "overall_answers = sorted(overall_answers, reverse=True)\n",
    "for count, answer in overall_answers:\n",
    "    print(answer + ': ' + str(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answers for the most frequent tuples accross different contexts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, entities, _ in sorted_answers[:100]:\n",
    "    print(entities, ':')\n",
    "    \n",
    "    res_res = []\n",
    "    for id_ in valid_ids:\n",
    "        if entities == ', '.join(sorted(database.queries[id_].entities)):\n",
    "            res = []\n",
    "            \n",
    "            for annotation in database.annotations[id_]:\n",
    "                if not annotation.bug:\n",
    "                    res.extend(annotation.preprocessed_answers)\n",
    "            \n",
    "            res = sorted(res)\n",
    "            res_res.append(res)\n",
    "            \n",
    "    flatten_res_res = [r for l in res_res for r in l]\n",
    "    count_res_res = sorted(set([(flatten_res_res.count(a), a) for a in flatten_res_res]), reverse=True)\n",
    "    \n",
    "    n = int(len(count_res_res)/2)\n",
    "    to_exclude = [a for _, a in count_res_res[:n]]\n",
    "    \n",
    "    for res in res_res:\n",
    "        res_excluded = [a if a not in to_exclude else '_' for a in res]\n",
    "        print(', '.join(set([r+' ['+str(res_excluded.count(r))+']' if res_excluded.count(r)>1 else r for r in res_excluded])))\n",
    "    \n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul de Kappa (Jeiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(database.annotations)\n",
    "print(\"Number of tasks (subjects): {}\".format(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "print(\"Number of annotation per task: {}\".format(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_bin = {0, 1}\n",
    "k_bin = len(categories_bin)\n",
    "print(\"Number of categories (binary case): {}\".format(k_bin))\n",
    "\n",
    "categories_gen = set()\n",
    "\n",
    "for id_, annotation_list in database.annotations.items():\n",
    "    for annotation in annotation_list:\n",
    "        if annotation.preprocessed_answers == []:\n",
    "            categories_gen.add('None')\n",
    "        else:\n",
    "            categories_gen.add(annotation.preprocessed_answers[0])\n",
    "\n",
    "k_gen = len(categories_gen)\n",
    "print(\"Number of catefories (genral case): {}\".format(k_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ij_bin = defaultdict(int)\n",
    "\n",
    "for id_, annotation_list in database.annotations.items():\n",
    "    for annotation in annotation_list:\n",
    "        if annotation.preprocessed_answers == []:\n",
    "            n_ij_bin[(id_, 0)] += 1\n",
    "        else:\n",
    "            n_ij_bin[(id_, 1)] += 1\n",
    "\n",
    "n_ij_gen = defaultdict(int)\n",
    "\n",
    "for id_, annotation_list in database.annotations.items():\n",
    "    for annotation in annotation_list:\n",
    "        if annotation.preprocessed_answers == []:\n",
    "            n_ij_gen[(id_, 'None')] += 1\n",
    "        else:\n",
    "            n_ij_gen[(id_, annotation.preprocessed_answers[0])] += 1\n",
    "\n",
    "pj_bin = dict([(category,\n",
    "               sum([n_ij_bin[(id_, category)] for id_ in database.annotations])/(N*n))\n",
    "               for category in categories_bin])\n",
    "\n",
    "pj_gen = dict([(category,\n",
    "               sum([n_ij_gen[(id_, category)] for id_ in database.annotations])/(N*n)\n",
    "               ) for category in categories_gen])\n",
    "\n",
    "Pi_bin = dict([(id_,\n",
    "               sum([n_ij_bin[(id_, category)]*(n_ij_bin[(id_, category)]-1) for category in categories_bin])/(n*(n-1))\n",
    "               ) for id_ in database.annotations])\n",
    "\n",
    "Pi_gen = dict([(id_,\n",
    "               sum([n_ij_gen[(id_, category)]*(n_ij_gen[(id_, category)]-1) for category in categories_gen])/(n*(n-1))\n",
    "               ) for id_ in database.annotations])\n",
    "\n",
    "Pm_bin = sum([Pi_bin[id_] for id_ in database.annotations])/N\n",
    "Pe_bin = sum([pj_bin[category]**2 for category in categories_bin])\n",
    "\n",
    "Pm_gen = sum([Pi_gen[id_] for id_ in database.annotations])/N\n",
    "Pe_gen = sum([pj_gen[category]**2 for category in categories_gen])\n",
    "\n",
    "kappa_bin = (Pm_bin-Pe_bin)/(1-Pe_bin)\n",
    "print(\"Kappa (binary case): {}\".format(kappa_bin))\n",
    "\n",
    "kappa_gen = (Pm_gen-Pe_gen)/(1-Pe_gen)\n",
    "print(\"Kappa (general case): {}\".format(kappa_gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of answers per annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotators = defaultdict(list)\n",
    "\n",
    "for id_, annotation_list in database.annotations.items():\n",
    "    for annotation in annotation_list:\n",
    "        annotators[annotation.worker_id].extend(annotation.preprocessed_answers)\n",
    "        \n",
    "n_annotators = len(annotators)\n",
    "mean = round(sum([len(l) for _, l in annotators.items()])/n_annotators)\n",
    "maximum = max([len(l) for _, l in annotators.items()])\n",
    "print(\"Number of annotators: {}; mean number of answers per annotators: {}; max: {}\".format(n_annotators, mean, maximum))\n",
    "\n",
    "annotators_all = sorted([(len(l), annotator) for annotator, l in annotators.items()], reverse=True)\n",
    "annotators_different = sorted([(len(set(l)), annotator) for annotator, l in annotators.items()], reverse=True)\n",
    "unique_answers = [answer for count, answer in overall_answers if count == 1]\n",
    "annotators_unique = sorted([(\n",
    "    len([answer for answer in l if answer in unique_answers]),\n",
    "    annotator) for annotator, l in annotators.items()], reverse=True)\n",
    "\n",
    "plt.figure(num=None, figsize=(16, 8))\n",
    "plt.bar(range(len(annotators)), [count for count, _ in annotators_all], width=0.95, label='Total number of answers per annotator')\n",
    "plt.bar(range(len(annotators)), [count for count, _ in annotators_different], width=0.95, label='Number of different answers per annotator')\n",
    "plt.bar(range(len(annotators)), [count for count, _ in annotators_unique], width=0.95, label='Number of unique (overall) answers per annotator')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequencies of the tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = defaultdict(int)\n",
    "\n",
    "for id_, annotation_list in database.annotations.items():\n",
    "    entities = tuple(database.queries[id_].entities)\n",
    "    for annotation in annotation_list:\n",
    "        frequencies[entities] += 1\n",
    "\n",
    "frequencies = sorted([(count, entities) for entities, count in frequencies.items()], reverse = True)\n",
    "n = len(frequencies)\n",
    "\n",
    "plt.figure(num=None, figsize=(16, 4))\n",
    "#plt.xscale(\"log\")\n",
    "plt.bar(range(1, n+1), [count for count, _ in frequencies], width=1)\n",
    "\n",
    "print(\"Number of tuples: {}\".format(len(frequencies)))\n",
    "\n",
    "for count, entities in frequencies:\n",
    "    print(', '.join(entities), ':', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = defaultdict(int)\n",
    "\n",
    "for id_, annotation_list in database.annotations.items():\n",
    "    entities = (database.queries[id_].entities_type_)\n",
    "    for annotation in annotation_list:\n",
    "        frequencies[entities] += 1\n",
    "\n",
    "frequencies = sorted([(count, entities) for entities, count in frequencies.items()], reverse = True)\n",
    "n = len(frequencies)\n",
    "\n",
    "plt.figure(num=None, figsize=(16, 4))\n",
    "#plt.xscale(\"log\")\n",
    "plt.bar(range(1, n+1), [count for count, _ in frequencies], width=1)\n",
    "\n",
    "print(\"Number of tuples: {}\".format(len(frequencies)))\n",
    "\n",
    "for count, entities in frequencies:\n",
    "    print(entities, ':', count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequencies of the bug tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies, frequencies_bug = defaultdict(int), defaultdict(int)\n",
    "\n",
    "for id_, annotation_list in database.annotations.items():\n",
    "    entities = tuple(database.queries[id_].entities)\n",
    "    for annotation in annotation_list:\n",
    "        frequencies[entities] += 1\n",
    "        \n",
    "        if annotation.bug:\n",
    "            frequencies_bug[entities] += 1\n",
    "        else:\n",
    "            frequencies_bug[entities] += 0.01\n",
    "\n",
    "frequencies_bug = sorted([(count/frequencies[entities], entities) for entities, count in frequencies_bug.items()], reverse = True)\n",
    "n_bug = len(frequencies_bug)\n",
    "\n",
    "plt.figure(num=None, figsize=(16, 4))\n",
    "#plt.xscale(\"log\")\n",
    "plt.bar(range(1, n_bug+1), [count for count, _ in frequencies_bug], width=1)\n",
    "\n",
    "for count, entities in frequencies_bug[:20]:\n",
    "    print(', '.join(entities), ':', count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequencies of the answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = defaultdict(int)\n",
    "\n",
    "for id_, annotation_list in database.annotations.items():\n",
    "    for annotation in annotation_list:\n",
    "        if not annotation.bug:\n",
    "            for answer in annotation.preprocessed_answers:\n",
    "                frequencies[answer] += 1\n",
    "\n",
    "frequencies = sorted([(count, answer) for answer, count in frequencies.items()], reverse = True)\n",
    "n = len(frequencies)\n",
    "\n",
    "plt.figure(num=None, figsize=(16, 4))\n",
    "#plt.xscale(\"log\")\n",
    "plt.bar(range(n), [count for count, _ in frequencies], width=1, log=True)\n",
    "\n",
    "for count, answer in frequencies[:20]:\n",
    "    print(answer, ':', count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of answers per task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = defaultdict(list)\n",
    "\n",
    "for id_, annotation_list in database.annotations.items():\n",
    "    for annotation in annotation_list:\n",
    "        answers[id_].extend(annotation.preprocessed_answers)\n",
    "                \n",
    "answers_all = [len(a) for _, a in answers.items()]\n",
    "answers_different = [len(set(a)) for _, a in answers.items()]\n",
    "answers_unique = [len([answer for answer in a if answer in unique_answers]) for _, a in answers.items()]\n",
    "\n",
    "bins = [i - 0.5 for i in range(8)]\n",
    "plt.figure(num=None, figsize=(5, 8))\n",
    "plt.hist(answers_all, bins, width=0.95, align='mid', label=\"Total number of answers per task\")\n",
    "plt.hist(answers_different, bins, width=.66, align='mid', label=\"Number of different answers per task\")\n",
    "plt.hist(answers_unique, bins, width=.33, align='mid', label=\"Number of unique (overall) answers per task\")\n",
    "plt.legend()\n",
    "plt.xticks(np.arange(min(bins)+0.5, max(bins)+0.5, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of answers per tuple of entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = defaultdict(list)\n",
    "\n",
    "for id_, annotation_list in database.annotations.items():\n",
    "    entities = tuple(sorted(database.queries[id_].entities))\n",
    "    for annotation in annotation_list:\n",
    "        answers[entities].extend(annotation.preprocessed_answers)\n",
    "                \n",
    "answers_all = [len(a) for _, a in answers.items()]\n",
    "answers_different = [len(set(a)) for _, a in answers.items()]\n",
    "answers_unique = [len([answer for answer in a if answer in unique_answers]) for _, a in answers.items()]\n",
    "\n",
    "bins = [i - 0.5 for i in range(20)]\n",
    "plt.figure(num=None, figsize=(20, 10))\n",
    "plt.hist(answers_all, bins, width=0.95, align='mid', label=\"Total number of answers per tuple of entities\")\n",
    "plt.hist(answers_different, bins, width=.66, align='mid', label=\"Number of different answers per tuple of entities\")\n",
    "plt.hist(answers_unique, bins, width=.33, align='mid', label=\"Number of unique (overall) answers per tuple of entities\")\n",
    "plt.legend()\n",
    "plt.xticks(np.arange(min(bins)+0.5, max(bins)+0.5, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean frequency of the answers per annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = defaultdict(list)\n",
    "\n",
    "for id_, annotation_list in database.annotations.items():\n",
    "    for annotation in annotation_list:\n",
    "        answers[annotation.worker_id].extend(annotation.preprocessed_answers)\n",
    "                \n",
    "answers_all = [np.mean([overall_answers_dict[answer] for answer in a]) for _, a in answers.items()]\n",
    "#answers_different = [len(set(a)) for _, a in answers.items()]\n",
    "#answers_unique = [len([answer for answer in a if answer in unique_answers]) for _, a in answers.items()]\n",
    "\n",
    "bins = [10*i  for i in range(13)]\n",
    "plt.figure(num=None, figsize=(20, 10))\n",
    "plt.hist(answers_all, bins, align='mid', label=\"Total number of answers per tuple of entities\")\n",
    "#plt.hist(answers_different, bins, width=.66, align='mid', label=\"Number of different answers per tuple of entities\")\n",
    "#plt.hist(answers_unique, bins, width=.33, align='mid', label=\"Number of unique (overall) answers per tuple of entities\")\n",
    "plt.legend()\n",
    "#plt.xticks(np.arange(min(bins)+0.5, max(bins)+0.5, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,auto:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

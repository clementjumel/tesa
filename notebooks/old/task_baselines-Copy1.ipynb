{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "del sys\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tesa.modeling.pipeline import Pipeline\n",
    "import tesa.modeling.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pipeline parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "drop_last = False\n",
    "test_proportion = 0.5\n",
    "valid_proportion = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()\n",
    "pipeline.process_data(batch_size=batch_size,\n",
    "                      drop_last=drop_last,\n",
    "                      test_proportion=test_proportion,\n",
    "                      valid_proportion=valid_proportion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_names = [\n",
    "    'average_precision', \n",
    "    'precision_at_k', \n",
    "    'recall_at_k', \n",
    "    'reciprocal_best_rank', \n",
    "    'reciprocal_average_rank', \n",
    "    'ndcg'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializes the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "word2vec_embedding = KeyedVectors.load_word2vec_format(fname='../modeling/pretrained_models/GoogleNews-vectors-negative300.bin',\n",
    "                                                       binary=True)\n",
    "\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple baselines\n",
    "## Random\n",
    "Completely random predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.RandomBaseline(scores_names=scores_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.preview_data(model=model, include_train=False, include_valid=True)\n",
    "pipeline.valid_model(model=model)\n",
    "model.display_metrics(scores_names=None)\n",
    "pipeline.explain_model(model=model, scores_names=None, n_samples=5, n_answers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency\n",
    "The prediction is based on the frequency of the choice as a good answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.FrequencyBaseline(scores_names=scores_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.preview_data(model=model, include_train=False, include_valid=True)\n",
    "pipeline.valid_model(model=model)\n",
    "model.display_metrics(scores_names=None)\n",
    "pipeline.explain_model(model=model, scores_names=None, n_samples=5, n_answers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary based\n",
    "## Summaries Counts\n",
    "The prediction is based on the total number of words (with repetition) of all the wikipedia summaries, that are in the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.SummariesCountBaseline(scores_names=scores_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.preview_data(model=model, include_train=False, include_valid=True)\n",
    "pipeline.valid_model(model=model)\n",
    "model.display_metrics(scores_names=None)\n",
    "pipeline.explain_model(model=model, scores_names=None, n_samples=5, n_answers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summaries Unique Count\n",
    "The prediction is based on the number of unique words of all the wikipedia summaries, that are in the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.SummariesUniqueCountBaseline(scores_names=scores_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.preview_data(model=model, include_train=False, include_valid=True)\n",
    "pipeline.valid_model(model=model)\n",
    "model.display_metrics(scores_names=None)\n",
    "pipeline.explain_model(model=model, scores_names=None, n_samples=5, n_answers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summaries Overlap\n",
    "The prediction is based on the number of (unique) words of the summaries' overlap that are in the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.SummariesOverlapBaseline(scores_names=scores_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.preview_data(model=model, include_train=False, include_valid=True)\n",
    "pipeline.valid_model(model=model)\n",
    "model.display_metrics(scores_names=None)\n",
    "pipeline.explain_model(model=model, scores_names=None, n_samples=5, n_answers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summaries Average Embedding\n",
    "The prediction if based on the (cosine) similarity between the average embedding of the words of the answer, and the average embedding of the words of all the wikipedia summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.SummariesAverageEmbeddingBaseline(scores_names=scores_names, \n",
    "                                          pretrained_model=word2vec_embedding,\n",
    "                                          pretrained_model_dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.preview_data(model=model, include_train=False, include_valid=True)\n",
    "pipeline.valid_model(model=model)\n",
    "model.display_metrics(scores_names=None)\n",
    "pipeline.explain_model(model=model, scores_names=None, n_samples=5, n_answers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summaries Overlap Average Embedding\n",
    "The prediction is based on the (cosine) similarity of the average embedding of the words of the answer and the average embedding of the overlap between all the wikipedia summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.SummariesOverlapAverageEmbeddingBaseline(scores_names=scores_names,\n",
    "                                                 pretrained_model=word2vec_embedding,\n",
    "                                                 pretrained_model_dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.preview_data(model=model, include_train=False, include_valid=True)\n",
    "pipeline.valid_model(model=model)\n",
    "model.display_metrics(scores_names=None)\n",
    "pipeline.explain_model(model=model, scores_names=None, n_samples=5, n_answers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activated Summaries\n",
    "The prediction is based on the number of summaries that have words matching a word from the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.ActivatedSummariesBaseline(scores_names=scores_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.preview_data(model=model, include_train=False, include_valid=True)\n",
    "pipeline.valid_model(model=model)\n",
    "model.display_metrics(scores_names=None)\n",
    "pipeline.explain_model(model=model, scores_names=None, n_samples=5, n_answers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context based\n",
    "## Context Counts\n",
    "The prediction is based on the total number of words (with repetition) of the context, that are in the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.ContextCountBaseline(scores_names=scores_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.preview_data(model=model, include_train=False, include_valid=True)\n",
    "pipeline.valid_model(model=model)\n",
    "model.display_metrics(scores_names=None)\n",
    "pipeline.explain_model(model=model, scores_names=None, n_samples=5, n_answers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Unique Count\n",
    "The prediction is based on the number of unique words of all the context, that are in the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.ContextUniqueCountBaseline(scores_names=scores_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.preview_data(model=model, include_train=False, include_valid=True)\n",
    "pipeline.valid_model(model=model)\n",
    "model.display_metrics(scores_names=None)\n",
    "pipeline.explain_model(model=model, scores_names=None, n_samples=5, n_answers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Average Embedding\n",
    "The prediction if based on the (cosine) similarity between the average embedding of the words of the answer, and the average embedding of the words of the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.ContextAverageEmbeddingBaseline(scores_names=scores_names, \n",
    "                                          pretrained_model=word2vec_embedding,\n",
    "                                          pretrained_model_dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.preview_data(model=model, include_train=False, include_valid=True)\n",
    "pipeline.valid_model(model=model)\n",
    "model.display_metrics(scores_names=None)\n",
    "pipeline.explain_model(model=model, scores_names=None, n_samples=5, n_answers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary & context based\n",
    "## Summaries Context Counts\n",
    "The prediction is based on the total number of words (with repetition) of all the wikipedia summaries and of the context, that are in the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.SummariesContextCountBaseline(scores_names=scores_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.preview_data(model=model, include_train=False, include_valid=True)\n",
    "pipeline.valid_model(model=model)\n",
    "model.display_metrics(scores_names=None)\n",
    "pipeline.explain_model(model=model, scores_names=None, n_samples=5, n_answers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summaries Context Unique Count\n",
    "The prediction is based on the number of unique words of all the wikipedia summaries and of the context, that are in the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.SummariesContextUniqueCountBaseline(scores_names=scores_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.preview_data(model=model, include_train=False, include_valid=True)\n",
    "pipeline.valid_model(model=model)\n",
    "model.display_metrics(scores_names=None)\n",
    "pipeline.explain_model(model=model, scores_names=None, n_samples=5, n_answers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summaries Overlap Context\n",
    "The prediction is based on the number of (unique) words of the summaries' overlap and of the context that are in the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.SummariesOverlapContextBaseline(scores_names=scores_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.preview_data(model=model, include_train=False, include_valid=True)\n",
    "pipeline.valid_model(model=model)\n",
    "model.display_metrics(scores_names=None)\n",
    "pipeline.explain_model(model=model, scores_names=None, n_samples=5, n_answers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summaries Context Average Embedding\n",
    "The prediction if based on the (cosine) similarity between the average embedding of the words of the answer, and the average embedding of the words of all the wikipedia summaries and of the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.SummariesContextAverageEmbeddingBaseline(scores_names=scores_names, \n",
    "                                          pretrained_model=word2vec_embedding,\n",
    "                                          pretrained_model_dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.preview_data(model=model, include_train=False, include_valid=True)\n",
    "pipeline.valid_model(model=model)\n",
    "model.display_metrics(scores_names=None)\n",
    "pipeline.explain_model(model=model, scores_names=None, n_samples=5, n_answers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summaries Overlap Context Average Embedding\n",
    "The prediction is based on the (cosine) similarity of the average embedding of the words of the answer and the average embedding of the overlap between all the wikipedia summaries and of the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.SummariesOverlapContextAverageEmbeddingBaseline(scores_names=scores_names,\n",
    "                                                 pretrained_model=word2vec_embedding,\n",
    "                                                 pretrained_model_dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.preview_data(model=model, include_train=False, include_valid=True)\n",
    "pipeline.valid_model(model=model)\n",
    "model.display_metrics(scores_names=None)\n",
    "pipeline.explain_model(model=model, scores_names=None, n_samples=5, n_answers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closest Summaries Embedding\n",
    "Grade is the cosine similarity between the average embedding of the words of an answer, and the average embedding of all the words of the wikipedia summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tesa.modeling.models import ClosestSoftOverlapEmbedding\n",
    "\n",
    "model = ClosestSoftOverlapEmbedding(scores_names=scores_names,\n",
    "                                    pretrained_model=word2vec_embedding,\n",
    "                                    pretrained_model_dim=300)\n",
    "pipeline.preview_data(model=model, include_valid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.valid_model(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.display_metrics(scores_names=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.explain_model(model=model, \n",
    "                       scores_names=None,\n",
    "                       n_samples=5,\n",
    "                       n_answers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling.models import BertEmbedding\n",
    "\n",
    "model = BertEmbedding(scores_names=scores_names,\n",
    "                      pretrained_model=bert_model,\n",
    "                      pretrained_model_dim=768,\n",
    "                      tokenizer=bert_tokenizer)\n",
    "\n",
    "pipeline.preview_data(model=model, include_valid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.valid_model(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.display_metrics(scores_names=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.explain_model(model=model, \n",
    "                       scores_names=None,\n",
    "                       n_samples=5,\n",
    "                       n_answers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT for Next Sentence Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tesa.modeling.models import NSPBertEmbedding\n",
    "\n",
    "model = NSPBertEmbedding(scores_names=scores_names)\n",
    "model.initialize_nsp_bert_embedding()\n",
    "pipeline.preview_data(model=model, include_valid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.valid_model(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.display_metrics(scores_names=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.explain_model(model=model, \n",
    "                       scores_names=None,\n",
    "                       n_samples=5,\n",
    "                       n_answers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenized input\n",
    "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "masked_index = 8\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "assert tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer', '[SEP]']\n",
    "\n",
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    "# We have a hidden states for each of the 12 layers in model bert-base-uncased\n",
    "assert len(encoded_layers) == 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_layers[11][:, 0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Allocate a pipeline for sentiment-analysis\n",
    "nlp = pipeline('sentiment-analysis')\n",
    "nlp('We are very happy to include pipeline into the transformers repository.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline('question-answering')\n",
    "nlp({\n",
    "    'question': 'What is the name of the repository ?',\n",
    "    'context': 'Pipeline have been included in the huggingface/transformers repository'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlp = pipeline('feature-extraction')\n",
    "nlp(\n",
    "    'the two politicians'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pipeline.valid_loader[0][0]\n",
    "outputs = pipeline.valid_loader[0][1]\n",
    "sequence_0 = features['context'] + ', '.join(features['entities']) + ' '.join(features['summaries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "\n",
    "classes = [\"not paraphrase\", \"is paraphrase\"]\n",
    "\n",
    "for i in range(len(features['choices'])):\n",
    "    sequence_1 = features['choices'][i]\n",
    "\n",
    "    paraphrase = tokenizer.encode_plus(sequence_0, sequence_1, return_tensors=\"pt\")\n",
    "    paraphrase_classification_logits = model(**paraphrase)[0]\n",
    "    paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "\n",
    "    if outputs[i]:\n",
    "        print(\"Should be high\")\n",
    "        print(f\"{classes[1]}: {round(paraphrase_results[1] * 100)}%\")    \n",
    "    else:\n",
    "        print(\"Should be low\")\n",
    "        print(f\"{classes[1]}: {round(paraphrase_results[1] * 100)}%\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

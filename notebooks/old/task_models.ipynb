{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "del sys\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard\n",
    "\n",
    "from tesa.modeling.pipeline import Pipeline\n",
    "from tesa.modeling.nn import RegressionMLP, ClassificationMLP, RegressionBilinear, ClassificationBilinear\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pipeline parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "drop_last = False\n",
    "test_proportion = 0.25\n",
    "valid_proportion = 0.25\n",
    "use_k_fold = False\n",
    "k_k_fold = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(use_k_fold=use_k_fold)\n",
    "pipeline.process_data(batch_size=batch_size,\n",
    "                      drop_last=drop_last,\n",
    "                      test_proportion=test_proportion,\n",
    "                      valid_proportion=valid_proportion,\n",
    "                      k=k_k_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_names = [\n",
    "    'average_precision', \n",
    "    'precision_at_k', \n",
    "    'recall_at_k', \n",
    "    'reciprocal_best_rank', \n",
    "    'reciprocal_average_rank', \n",
    "    'ndcg'\n",
    "]\n",
    "n_updates = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tesa.modeling.models import BaseModel\n",
    "BaseModel.initialize_word2vec_embedding()\n",
    "BaseModel.initialize_bert_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Half BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim, hidden_dim1, hidden_dim2 = 5537, 1000, 100\n",
    "\n",
    "dropout = 0.1\n",
    "lr = 4e-7\n",
    "milestones = [1, 2, 6]\n",
    "gamma = 0.5\n",
    "n_epochs = 10\n",
    "\n",
    "is_regression = True\n",
    "#is_regression = False\n",
    "\n",
    "if is_regression:\n",
    "    loss = torch.nn.MSELoss()\n",
    "    net = RegressionMLP(input_dim=input_dim, hidden_dim1=hidden_dim1, hidden_dim2=hidden_dim2, dropout=dropout)\n",
    "    \n",
    "else:\n",
    "    weight = torch.tensor([1, 1], dtype=torch.float)\n",
    "    loss = torch.nn.CrossEntropyLoss(weight=weight)\n",
    "    net = ClassificationMLP(input_dim=input_dim, hidden_dim1=hidden_dim1, hidden_dim2=hidden_dim2, dropout=dropout)\n",
    "    \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=milestones, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tesa.modeling.models import HalfBOWModel\n",
    "\n",
    "model = HalfBOWModel(scores_names=scores_names, \n",
    "                     experiment_name='test_1',\n",
    "                     net=net,\n",
    "                     optimizer=optimizer,\n",
    "                     lr_scheduler=lr_scheduler,\n",
    "                     loss=loss,\n",
    "                     vocab_frequency_range=[100, 10000000000])\n",
    "\n",
    "pipeline.preview_data(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.train_model(model=model, \n",
    "                     n_epochs=n_epochs, \n",
    "                     n_updates=n_updates,\n",
    "                     is_regression=is_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.train_model(model=model, \n",
    "                     n_epochs=n_epochs, \n",
    "                     n_updates=n_updates,\n",
    "                     is_regression=is_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.final_plot(align_experiments=True,\n",
    "                 display_training_scores=False, \n",
    "                 scores_names=[\n",
    "                     'average_precision', \n",
    "                     'precision_at_k', \n",
    "                     'recall_at_k', \n",
    "                     'reciprocal_best_rank', \n",
    "                     'reciprocal_average_rank', \n",
    "                     'ndcg'\n",
    "                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.display_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.explain_model(model=model, \n",
    "                       display_explanations=True,\n",
    "                       n_samples=5,\n",
    "                       n_answers=10,\n",
    "                       scores_names=[\n",
    "                           'average_precision', \n",
    "                           'precision_at_k', \n",
    "                           'recall_at_k', \n",
    "                           'reciprocal_best_rank', \n",
    "                           'reciprocal_average_rank', \n",
    "                           'ndcg'\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim, hidden_dim1, hidden_dim2 = 3196, 1000, 100\n",
    "\n",
    "dropout = 0.1\n",
    "lr = 4e-7\n",
    "milestones = [1, 2, 5, 8]\n",
    "gamma = 0.5\n",
    "n_epochs = 10\n",
    "\n",
    "is_regression = False\n",
    "\n",
    "if is_regression:\n",
    "    loss = torch.nn.MSELoss()\n",
    "    net = RegressionMLP(input_dim=input_dim, hidden_dim1=hidden_dim1, hidden_dim2=hidden_dim2, dropout=dropout)\n",
    "    \n",
    "else:\n",
    "    weight = torch.tensor([1, 1], dtype=torch.float)\n",
    "    loss = torch.nn.CrossEntropyLoss(weight=weight)\n",
    "    net = ClassificationMLP(input_dim=input_dim, hidden_dim1=hidden_dim1, hidden_dim2=hidden_dim2, dropout=dropout)\n",
    "    \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=milestones, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling.models import FullBOWModel\n",
    "\n",
    "model = FullBOWModel(vocab_frequency_range=[100, 10000],\n",
    "                     net=net,\n",
    "                     optimizer=optimizer,\n",
    "                     lr_scheduler=lr_scheduler,\n",
    "                     loss=loss,\n",
    "                     scores_names=scores_names)\n",
    "\n",
    "pipeline.preview_data(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.train_model(model=model, \n",
    "                     n_epochs=n_epochs, \n",
    "                     n_updates=n_updates,\n",
    "                     is_regression=is_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.final_plot(align_experiments=True,\n",
    "                 display_training_scores=False, \n",
    "                 scores_names=[\n",
    "                     'average_precision', \n",
    "                     'precision_at_k', \n",
    "                     'recall_at_k', \n",
    "                     'reciprocal_best_rank', \n",
    "                     'reciprocal_average_rank', \n",
    "                     'ndcg'\n",
    "                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.display_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.explain_model(model=model, \n",
    "                       display_explanations=True,\n",
    "                       n_samples=5,\n",
    "                       n_answers=10,\n",
    "                       scores_names=[\n",
    "                           'average_precision', \n",
    "                           'precision_at_k', \n",
    "                           'recall_at_k', \n",
    "                           'reciprocal_best_rank', \n",
    "                           'reciprocal_average_rank', \n",
    "                           'ndcg'\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim, hidden_dim1, hidden_dim2 = 600, 1000, 100\n",
    "\n",
    "dropout = 0.1\n",
    "lr = 4e-7\n",
    "milestones = [1, 2, 5, 8]\n",
    "gamma = 0.5\n",
    "n_epochs = 10\n",
    "\n",
    "#is_regression = False\n",
    "is_regression = True\n",
    "\n",
    "if is_regression:\n",
    "    loss = torch.nn.MSELoss()\n",
    "    net = RegressionMLP(input_dim=input_dim, hidden_dim1=hidden_dim1, hidden_dim2=hidden_dim2, dropout=dropout)\n",
    "    \n",
    "else:\n",
    "    weight = torch.tensor([1, 1], dtype=torch.float)\n",
    "    loss = torch.nn.CrossEntropyLoss(weight=weight)\n",
    "    net = ClassificationMLP(input_dim=input_dim, hidden_dim1=hidden_dim1, hidden_dim2=hidden_dim2, dropout=dropout)\n",
    "    \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=milestones, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tesa.modeling.models import EmbeddingModel\n",
    "\n",
    "model = EmbeddingModel(scores_names=scores_names,\n",
    "                       net=net,\n",
    "                       optimizer=optimizer,\n",
    "                       lr_scheduler=lr_scheduler,\n",
    "                       loss=loss,\n",
    "                       experiment_name='test_2')\n",
    "\n",
    "pipeline.preview_data(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.train_model(model=model, \n",
    "                     n_epochs=n_epochs, \n",
    "                     n_updates=n_updates,\n",
    "                     is_regression=is_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.final_plot(align_experiments=True,\n",
    "                 display_training_scores=False, \n",
    "                 scores_names=[\n",
    "                     'average_precision', \n",
    "                     'precision_at_k', \n",
    "                     'recall_at_k', \n",
    "                     'reciprocal_best_rank', \n",
    "                     'reciprocal_average_rank', \n",
    "                     'ndcg'\n",
    "                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.display_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.explain_model(model=model, \n",
    "                       display_explanations=True,\n",
    "                       n_samples=5,\n",
    "                       n_answers=10,\n",
    "                       scores_names=[\n",
    "                           'average_precision', \n",
    "                           'precision_at_k', \n",
    "                           'recall_at_k', \n",
    "                           'reciprocal_best_rank', \n",
    "                           'reciprocal_average_rank', \n",
    "                           'ndcg'\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Bilinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim1, input_dim2 = 300, 300\n",
    "\n",
    "dropout = 0.1\n",
    "lr = 8e-7\n",
    "milestones = [1, 3]\n",
    "gamma = 0.5\n",
    "n_epochs = 2\n",
    "\n",
    "#is_regression = False\n",
    "is_regression = True\n",
    "\n",
    "if is_regression:\n",
    "    loss = torch.nn.MSELoss()\n",
    "    net = RegressionBilinear(input_dim1=input_dim1, input_dim2=input_dim2, dropout=dropout)\n",
    "    \n",
    "else:\n",
    "    weight = torch.tensor([1, 1], dtype=torch.float)\n",
    "    loss = torch.nn.CrossEntropyLoss(weight=weight)\n",
    "    net = ClassificationBilinear(input_dim1=input_dim1, input_dim2=input_dim2, dropout=dropout)\n",
    "    \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=milestones, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tesa.modeling.models import EmbeddingBilinearModel\n",
    "\n",
    "model = EmbeddingBilinearModel(scores_names=scores_names,\n",
    "                               net=net,\n",
    "                               optimizer=optimizer,\n",
    "                               lr_scheduler=lr_scheduler,\n",
    "                               loss=loss,\n",
    "                               experiment_name='test_3')\n",
    "\n",
    "pipeline.preview_data(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/test_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.train_model(model=model, \n",
    "                     n_epochs=n_epochs, \n",
    "                     n_updates=n_updates,\n",
    "                     is_regression=is_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.train_model(model=model, \n",
    "                     n_epochs=n_epochs, \n",
    "                     n_updates=n_updates,\n",
    "                     is_regression=is_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.final_plot(align_experiments=True,\n",
    "                 display_training_scores=False, \n",
    "                 scores_names=[\n",
    "                     'average_precision', \n",
    "                     'precision_at_k', \n",
    "                     'recall_at_k', \n",
    "                     'reciprocal_best_rank', \n",
    "                     'reciprocal_average_rank', \n",
    "                     'ndcg'\n",
    "                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.display_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.explain_model(model=model, \n",
    "                       display_explanations=True,\n",
    "                       n_samples=5,\n",
    "                       n_answers=10,\n",
    "                       scores_names=[\n",
    "                           'average_precision', \n",
    "                           'precision_at_k', \n",
    "                           'recall_at_k', \n",
    "                           'reciprocal_best_rank', \n",
    "                           'reciprocal_average_rank', \n",
    "                           'ndcg'\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
